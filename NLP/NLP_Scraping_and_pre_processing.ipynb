{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9Sx__LDsv-il"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download and process the dataset"
      ],
      "metadata": {
        "id": "y9XMWmWTuUrZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUhb1o8aRZvR"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install spotdl\n",
        "!pip install ffmpeg-python\n",
        "\n",
        "# Download FFmpeg and set up PATH\n",
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "# import ffmpeg\n",
        "# import spotdl\n",
        "# from spotdl import Spotdl\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Mn4pW00zkhMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive for dataset\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD693wiyYcjt",
        "outputId": "d852ebb5-4690-4777-8922-a75d0ee9e148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database_path = r'/content/gdrive/MyDrive/Musicoset/'\n",
        "path = r'/content/gdrive/MyDrive/Musicoset/songs.csv'\n",
        "save_path = r'/content/gdrive/MyDrive/Musicoset/songs/%(title)s.%(ext)s'\n",
        "\n",
        "if os.path.exists == False:\n",
        "  os.makedirs(save_path)"
      ],
      "metadata": {
        "id": "ub44FXEdYovp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the count of the data\n",
        "\n",
        "data = pd.read_csv(path, sep=\"\\t\")\n",
        "explicit_true_count = len([1 for i in data['explicit'] if i ==True])\n",
        "explicit_false_count = len([1 for i in data['explicit'] if i ==False])\n",
        "print(explicit_true_count, explicit_false_count)"
      ],
      "metadata": {
        "id": "gtHlFJ3cZCNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# balance out the samples between explicit == True and explicit == False\n",
        "true_explicit = data[data['explicit']==True]\n",
        "false_explicit = data[data['explicit']== False]\n",
        "false_explicit_sample = data.sample(n=explicit_true_count, replace=False, random_state=1).reset_index()\n",
        "false_explicit_sample = false_explicit_sample.iloc[:,1:]\n",
        "balanced_data = pd.concat([true_explicit, false_explicit_sample]).reset_index(drop=True)\n",
        "# balanced_data = balanced_data.iloc[1159:, :]"
      ],
      "metadata": {
        "id": "WjbOvB8BkejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add song name to the dataset\n",
        "balanced_data['song_name'] = ''\n",
        "for i, val in enumerate(balanced_data['billboard']):\n",
        "  val = eval(val)\n",
        "  song, artist = val[0], val[1]\n",
        "  song_name = f\"{artist} - {song}\"\n",
        "  balanced_data.loc[i, 'song_name'] = song_name"
      ],
      "metadata": {
        "id": "AjZvUst2KwVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the csv file\n",
        "csv_save_path = os.path.join(database_path, 'balanced_data.csv')\n",
        "if not os.path.exists(csv_save_path):\n",
        "  balanced_data.to_csv(csv_save_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "a_PLenkOA163"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scraping\n",
        "\n",
        "for i, n in enumerate(balanced_data['song_id']):\n",
        "  track_link = 'https://open.spotify.com/track/{}'.format(n)\n",
        "  os.system(f'spotdl {track_link} --output \"{save_path}\"')\n",
        "  print(f\"track no. {i} is saved\")\n"
      ],
      "metadata": {
        "id": "z5JVJ6T0g9Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since the name in the dataset and the result of scraping is different,\n",
        "# I incorporate the similarity algorithm to match both names, and rename it.\n",
        "\n",
        "def similarity(sentence1, sentence2, vectorizer):\n",
        "    tfidf_matrix = vectorizer.transform([sentence1, sentence2])\n",
        "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "def get_similar_sentences(df, queries):\n",
        "    vectorizer = TfidfVectorizer().fit(df['song_name'].str.lower())\n",
        "    df_copy = df.copy()\n",
        "    results = {}\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        query = query.lower()\n",
        "        similarities = df['song_name'].apply(lambda x: similarity(query, x.lower(), vectorizer))\n",
        "        most_similar_idx = similarities.idxmax()\n",
        "        most_similar_score = similarities.max()\n",
        "        results[query] = [(df.iloc[most_similar_idx]['song_name'], most_similar_score)]\n",
        "        df_copy.at[most_similar_idx, 'song_name'] = query\n",
        "        print(i)\n",
        "        print(query)\n",
        "        print(results[query])\n",
        "\n",
        "    return results, df_copy\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'D:/NLP final project/balanced_data.csv')\n",
        "path = r'D:/NLP final project\\songs\\whisper\\whisper_dataset_np'\n",
        "path_label = r'D:/NLP final project/balanced_data.csv'\n",
        "pth = r'D:/NLP final project'\n",
        "\n",
        "\n",
        "queries = [x for x in os.listdir(path)]\n",
        "similar_sentences, updated_df = get_similar_sentences(df, queries)\n",
        "updated_df.to_csv(os.path.join(pth, 'balanced_data_new.csv'), index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "DjgYPja50Ny8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data processing (with background music)"
      ],
      "metadata": {
        "id": "egreTDbXVkj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W5FKpt4mSH4",
        "outputId": "096631a7-ae3f-41f1-e62e-e1c6e1274ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "63tAQZK3Yl6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vQNVO3SoUzR",
        "outputId": "c830d2d1-3739-4312-8199-99efbfecc51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ast_np_path = r'/content/drive/MyDrive/Musicoset/ast_dataset_np_correct'\n",
        "audio_path = r'/content/drive/MyDrive/Musicoset/songs/%(title)s.%(ext)s'\n",
        "csv_path = r'/content/drive/MyDrive/Musicoset/df_final.csv'\n",
        "if os.path.exists(ast_np_path) == False:\n",
        "  os.makedirs(ast_np_path)"
      ],
      "metadata": {
        "id": "6PgOXlNqYVLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "name_list = [x[:-4] for x in df['song_name']]"
      ],
      "metadata": {
        "id": "bRY39v6AkB1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def audio_sampling(audio_path, song, sampling_rate, n_mels,cut_rate, model='ast'):\n",
        "\n",
        "  # load mp3 file with librosa\n",
        "  y, _ = librosa.load(os.path.join(audio_path, song))\n",
        "\n",
        "  # sampling\n",
        "  sample = librosa.feature.melspectrogram(y=y, n_mels=n_mels, sr=sampling_rate, n_fft=400, hop_length=160, pad_mode='constant')\n",
        "  sample_log = librosa.power_to_db(sample, ref=np.max)\n",
        "\n",
        "  # finding the possible amount of sample of one song based on time resolution\n",
        "  x = math.ceil(sample_log.shape[-1] / cut_rate)\n",
        "\n",
        "  # padding with zero\n",
        "  if sample_log.shape[-1] < (x * cut_rate):\n",
        "    length_dev = abs(sample_log.shape[-1] - (x * cut_rate))\n",
        "    width = [(0, 0)] * (sample_log.ndim - 1) + [(0, length_dev)]\n",
        "    sample_log = np.pad(sample_log, width, mode= \"constant\")\n",
        "  else: sample_log = sample_log\n",
        "\n",
        "  # stacking the samples on the first dimension to simplify processing\n",
        "  sample_split_log = np.hsplit(sample_log, x)\n",
        "  sample_split_log_stack = np.stack(sample_split_log, 0)\n",
        "\n",
        "  # swap the second and third dimension as the input of Audio spectrogram Transformer\n",
        "  if model == 'ast':\n",
        "    sample_split_log_stack = np.einsum('abc -> acb', sample_split_log_stack)\n",
        "\n",
        "  return sample_split_log_stack\n",
        "\n",
        "# save audio file\n",
        "def save_audio(audio_path, name, files):\n",
        "  filename = name + '.npy'\n",
        "  np.save(os.path.join(audio_path, filename), files)\n",
        "\n"
      ],
      "metadata": {
        "id": "xtxow7lgX7qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of songs with AST\n",
        "\n",
        "sampling_rate = 16000\n",
        "cut_rate = 3000\n",
        "count = 0\n",
        "n_mels = 128\n",
        "for song in os.listdir(audio_path):\n",
        "  filename, filetype = os.path.splitext(song)\n",
        "  filename = filename.lower()\n",
        "  if filename in name_list:\n",
        "    x = audio_sampling(audio_path, song, sampling_rate, n_mels, cut_rate, model='ast')\n",
        "    print(x.shape)\n",
        "    print(count)\n",
        "    count += 1\n",
        "    save_audio(ast_np_path, filename, x)\n",
        "  else:\n",
        "    print(f'song {filename} is not in name_list')\n",
        "    continue\n",
        "print('process finished')"
      ],
      "metadata": {
        "id": "GFu9fQhcYyu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}